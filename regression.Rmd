---
title: "Regression"
author: "Sahana Rayan"
date: "June 22, 2020"
output: html_document
---

#Loading the dataset
```{r message = FALSE, error=FALSE, warning=FALSE}
library(tidyverse)
tennis_data <- read_csv("http://www.stat.cmu.edu/cmsac/sure/materials/data/regression_projects/tennis_2013_2017_GS.csv")
```

#Cleaning the dataset
When looking at the composition of certain variables, lack of variation in these variables was identified.
```{r}
table("Retirement Binary Variable" = tennis_data$Retirement)
table("Winning player's sets won" = tennis_data$w_setswon)
```
 
Some variables were also redundant, as shown below, that required to be removed

```{r}
table("Slam" = tennis_data$slam,
      "Tournament" = tennis_data$tournament)
```


This information helped in creating this cleaned dataframe
```{r}
tennis_data_cleaned <- tennis_data %>% 
  select(-c("w_n_netpt_w", "w_n_netpt", "slam", "player1", "player2", "a1", "a2", "X1", "match_id.y")) %>% 
  rename(Net_points = l_n_netpt,
         Net_points_won = l_n_netpt_w) %>% 
  mutate(w_bp_success = w_n_bp_w/w_n_bp,
         l_bp_success = l_n_bp_w/l_n_bp,
         w_sv_success = w_n_sv_w/w_n_sv,
         l_sv_success = l_n_sv_w/l_n_sv,
         upset_bool = ifelse(winner_rank >loser_rank, TRUE, FALSE)) %>% 
  filter(Retirement == FALSE) %>% 
  filter(w_setswon %in% c(2,3))
```

The __select__ line removed all the redundant variables. The __rename__ function was used to rename certain variables to fit the context of the dataframe. The __mutate__ function was used to add some variables that we though couldbe worth looking at for EDA. These variables are serve success, break point success and a boolean variable that determines whether that particular match was an upset match. The __filter__ was used to filter out data that lacked variation.


#EDA for continuous variables
The correlation matrix was used to look at the linear relationships between certain continuous variables that we were interested in

```{r}
library(ggcorrplot)
tennis_model_data <- tennis_data_cleaned %>%
  dplyr::select(w_n_winners, #winning player's number of winners
                w_bp_success,#Winning player's breakpoint success
                w_sv_success,#Winning player's serve success
                w_pointswon, #Winning player's points won
                w_n_ue,      #Winning player's number of unforced errors
                l_n_ue,      #Losing player's number of unforced errors
                w_n_sv_w,    #Winning player's number of serve wins
                w_gameswon)  #Winning player's number of games won
tennis_cor_matrix <- cor(tennis_model_data)
ggcorrplot(tennis_cor_matrix)

round_cor_matrix <- 
  round(cor(tennis_model_data), 2)
ggcorrplot(round_cor_matrix, 
           hc.order = TRUE,
           type = "lower",
           lab = TRUE,
           colors = c("#0072B2", "white", "#D55E00")) +
  labs(
    title = "Correlation Plot with variables of interest"
  ) +
  scale_color_continuous()

```

The losing player's number of unforced errors, winning player's number of winners, winning player's games won, winning player's number of serve wins, and winning player's number of unforced errors had a strong correlation with winning player's total points won

*w_pointswon* appeared to be an interesting variable and a potential response variable for the linear regression model and so the distrubution of this variable looked like this
```{r}
tennis_data_cleaned %>% 
  ggplot() +
  geom_histogram(aes(x = w_pointswon)) +
  xlab("Winning player's points won") +
  theme_bw()
```


The distribution appears to have some outliers and once these outliers are removed, the distribution looks like this:
```{r}
tennis_data_cleaned %>% 
  filter((w_pointswon > 50) & (w_pointswon < 200)) %>%
  ggplot(aes(x = w_pointswon)) +
  geom_histogram(aes(y = ..density..), fill = "#0072B2", color = "black") +
  geom_density(color = "#D55E00", size = 1.5) +
  xlab("Winning player's points won") +
  ylab("Density") +
  labs(
    title = "Histogram of the density of Winning player's total points won"
  ) +
  theme_bw()
tennis_data_linear <- tennis_data_cleaned %>% 
  filter((w_pointswon > 50) & (w_pointswon < 200))

```

The **tennis_data_linear** is a dataframe containing data with the outliers in the *w_pointswon* distribution removed. This dataframe will be used in the rest of the project. 

The scatterplots for the aforementioned variables against *w_pointswon* were plotted


```{r messsage = FALSE}
stat.lab <- c("Losing player's no. of unforced errors", "Winning player's no. of serve wins", 
              "Winning player's no. of unforced errors", "Winning player's no. of winners",
              "Winning player's games won")
names(stat.lab) <- c("l_n_ue", "w_n_sv_w", "w_n_ue", "w_n_winners", "w_gameswon")
tennis_data_linear%>% 
  pivot_longer(c("w_n_ue", "w_n_winners", "l_n_ue", "w_n_sv_w", "w_gameswon"),
               names_to = "Stat", 
               values_to = "Values") %>% 
  ggplot(aes(x = Values, y = w_pointswon)) +
  geom_point(alpha = .2) +
  facet_wrap(~Stat, scales = "free_x", labeller = labeller(Stat = stat.lab)) +
  geom_smooth(method = "lm", color = "#0072B2") +
  ylab("Winning player's total points won") +
  theme_bw() +
  labs(
    title = "Different variables vs Winning player's points won scatterplot"
  )

tennis_data_linear %>% 
  group_by(w_gameswon) %>% 
  summarise(mean_pointswon = mean(w_pointswon)) %>% 
  ggplot() +
  geom_col(aes(x = w_gameswon, y = mean_pointswon),  stat = "identity")
```

__Note: the winning player's games won will be made into a bar chart because of its discrete nature__

These promising scatter plots led us to select *w_pointswon* as a response variable.


But, in order to look at linear relationships among explanatory variables, pairs plots were used. This is important in detecting any redundancies within the model as in an ideal situation, all explanatory variables are independent of one and other.

```{r message = FALSE}
library(GGally)
ggpairs(tennis_data_linear,
        columns =
          c("w_n_ue", "l_n_ue", "w_n_winners", "w_n_sv_w", "w_gameswon"),
        mapping = aes(alpha = 0.2)) +
  theme_bw()

```

#Hierarchial Clustering of variables

Hierarchial Clustering of variables is also a great way to group variables that are similar or variables that shows dependencies on each other. Other continuous variables (variables in the correlation matrix) were also included in this clustering model.

```{r}
tennis_ex_vars <- dplyr::select(tennis_model_data, -w_pointswon)
exp_cor_matrix <- cor(tennis_ex_vars)
cor_dist_matrix <- 1 - abs(exp_cor_matrix)
cor_dist_matrix <- as.dist(cor_dist_matrix)

library(ggdendro)
tennis_exp_hc <- hclust(cor_dist_matrix,
                     "complete")
ggdendrogram(tennis_exp_hc,
             rotate = TRUE,
             size = 2) +
  labs(
    title = "Dendrogram for selected variables"
  )
             
#library(dendextend)
#cor_dist_matrix %>%
  #hclust() %>%
  #as.dendrogram() %>%
  #set("branches_k_col", 
     # k = 2) %>% 
  #set("labels_cex", .9) %>%
  #ggplot(horiz = TRUE)

```

Both the dendrogram and the pairs plot indicate that *w_n_sv_w*,  *w_n_winners*, and *w_gameswon* have a high correlation with each other and when they are included in the linear regression model, these variables might create some redundancy.


#EDA for Discrete Variables

_round_, _tour_, _Tournament_, _Year_, *w_setswon*, are a couple of categorical variables that could potentially affect *w_pointswon*

Box plots along with violin plots were useful in determining this relationship
```{r}
tennis_data_linear %>% 
  ggplot(aes(x = as.factor(w_setswon), y = w_pointswon)) +
  geom_violin(color = "#0072B2", size = 1.3) +
  geom_boxplot(width = .2, color = "#D55E00", size = .7) + 
  xlab("Winning player's number of sets won") +
  ylab("Winning player's Total points won") +
  labs(title = "Violin and Box plots for sets won and points won") +
  theme_bw()
```


As expected, there seems to be a relationship between total points won and number of sets won.

```{r}
tennis_data_linear %>%
  ggplot(aes(x = Tour, y = w_pointswon)) +
  geom_violin(color = "#0072B2", size = 1.3) +
  geom_boxplot(width = .2, color = "#D55E00", size = .7) + 
  xlab("Tour") +
  ylab("Winning player's Total points won") +
  labs(title = "Violin and Box plots for Women's/Men's league and points won") +
  scale_x_discrete(limits = c("wta", "atp"), 
                   label = c("Womens Tennis Association", "Association for Tennis Players (Men's)")) +
  theme_bw()
```

There is distinct difference in distribution of *w_pointswon* in the Men's and Women's league of tennis.

These clear differences cannot be observed with _tournament_, _round_, and _Year_ as shown below with _tournament_ variable

```{r}
tennis_data_linear %>%
  ggplot(aes(x = tournament, y = w_pointswon)) +
  geom_violin(color = "#0072B2", size = 1.3) +
  geom_boxplot(width = .2, color = "#D55E00", size = .7) + 
  xlab("Tournament") +
  ylab("Winning player's Total points won") +
  labs(title = "Violin and Box plots for different tournaments and points won") +
  theme_bw()
```


#Elastic Net Regularization

In order to confirm the results of the variable selection done through EDA, Elastic Net regularization is performed. 

First, the optimal alpha or the alpha value that gives the least cv error is found. Models with alpha values between 0 and 1 with increments of 0.1 are trained

```{r}
library(glmnet)
tennis_data_linear <- tennis_data_linear %>% na.omit()
model_x <- model.matrix(w_pointswon ~ ., tennis_data_linear)[, -c(1, 2)]
model_y <- tennis_data_linear$w_pointswon
set.seed(2020)
fold_id <- sample(rep(1:10, length.out = nrow(model_x)))
alphalist <- seq(0,1,by=0.1)
elasticnet <- lapply(alphalist, function(a){
  cv.glmnet(model_x, model_y, alpha=a, foldid = fold_id)
})
```

The minimum cv errors are printed for each of the 11 models and the model with the least cv error and the coeffecients of this linear model was printed. This is done to check if the variables that the model keeps corresponds to the variables selected through EDA

```{r}
elastic <- elasticnet[[5]]
#coef(elastic)
elastic$lambda.min
```
#Linear Modeling

Explanatory Variables of interest include *w_n_winners*, *w_n_ue*, *Tour*, *l_n_ue*, *w_gameswon*
Response Variiable in question is *w_pointswon*

*w_setswon* was removed because it didn't give much variation from *Tour*.
```{r}
table("tour" = tennis_data_linear$Tour,
      "sets won" = as.factor(tennis_data_linear$w_setswon))
```

Three different models have been created. One model is with *tournament* and one model is without *tournament* along with a model having an interaction between *tournament* and *Tour*. A null model was also used as a comparison point in this analysis. K fold cross validation was performed and the root mean squared errors were plotted. 

The function for k-fold cross validation was created
```{r}
set.seed(2020)
tennis_data_linear <- tennis_data_linear %>%
  mutate(test_fold = sample(rep(1:5, length.out = n())))
get_cv_preds <- function(model_formula, data = tennis_data_linear) {
  # generate holdout predictions for every row based season
  map_dfr(unique(data$test_fold), 
          function(holdout) {
            # Separate test and training data:
            test_data <- data %>%
              filter(test_fold == holdout)
            train_data <- data %>%
              filter(test_fold != holdout)
            # Train model:
            reg_model <- lm(as.formula(model_formula), data = train_data)
            # Return tibble of holdout results:
            tibble(test_preds = predict(reg_model, newdata = test_data),
                   test_actual = test_data$w_pointswon,
                   test_fold = holdout) 
          })
}
```

The models were trained using K fold validation and these are the MSEs for the three models

```{r}
all_cv_preds <- get_cv_preds("w_pointswon ~ w_n_winners + Tour + tournament + w_n_ue + l_n_ue + w_gameswon")
no_tournament_cv_preds <- get_cv_preds("w_pointswon ~ w_n_winners + Tour + w_n_ue + l_n_ue + w_gameswon")
interaction_cv_preds <- get_cv_preds("w_pointswon ~ w_n_winners + Tour*tournament + w_n_ue + l_n_ue + w_gameswon")
null_cv_preds <- get_cv_preds("w_pointswon ~ 1")


bind_rows(mutate(all_cv_preds, type = "All"),
          mutate(no_tournament_cv_preds, type = "Excludes Tournament"),
          mutate(interaction_cv_preds, type = "Interaction between Tour & Tournament"),
          mutate(null_cv_preds, type = "Null Model")) %>%
  group_by(type) %>%
  summarize(rmse = sqrt(mean((test_actual - test_preds)^2))) %>%
  mutate(type = fct_reorder(type, rmse)) %>%
  ggplot(aes(x = type, y = rmse)) +
  geom_point(color = "#D55E00") + coord_flip() + theme_bw() + xlab("Model Type") + ylab("Root Mean Squared Error")
```

All the three models have a better prediction accuracy than the null model. The model with all selected variables and no interactions had the lowest RMSE. 

#Interpretation of this model
The model's summary is displayed below along with its coeffecient plot.
```{r}
library(GGally)
all_model <- lm(w_pointswon ~ w_n_winners + Tour + tournament + w_n_ue + l_n_ue + w_gameswon,
                data = tennis_data_linear)
summary(all_model)
ggcoef(all_model, 
       exclude_intercept = TRUE,
       vline = TRUE,
       vline_color = "red") + 
  theme_bw() +
  labs(title = "All Variables model") +
  xlab("Coefficients") +
  ylab("Variables")
```


Coeffecient analysis can be done better if all variables in the model are scaled.

```{r}
coef_analysis_model <- lm(scale(w_pointswon) ~ scale(w_n_winners) + Tour + tournament + 
                            scale(w_n_ue) + scale(l_n_ue) + scale(w_gameswon),
                          data = tennis_data_linear)

ggcoef(coef_analysis_model, 
       exclude_intercept = TRUE,
       vline = TRUE,
       vline_color = "red") + 
  theme_bw() +
  labs(title = "Scaled Variables model") +
  xlab("Coefficients") +
  ylab("Variables")
```
These are the Diagnostic plots for this model. 
```{r}
library(ggfortify)
autoplot(all_model, 
         which = c(1, 2),
         alpha = .2,
         ncols = 2) +
  theme_bw()

autoplot(all_model, 
         which = c(3, 4),
         alpha = .2,
         ncols = 2) +
  theme_bw()
autoplot(all_model, 
         which = c(5, 6),
         alpha = .2,
         ncols = 2) +
  theme_bw()
```